{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wEsTOyLwnEXR"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nqiu7jxS9SWX"
      },
      "outputs": [],
      "source": [
        "! pip install hazm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S5RlBpzhM0Um"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "from hazm import *\n",
        "from matplotlib import pyplot as plt\n",
        "import json\n",
        "#from parsivar import *\n",
        "import operator\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g4BLUkaxj_PE"
      },
      "outputs": [],
      "source": [
        "punctuation = [':', '،', '.', ')', '(', '}', '{', '؟', '!', '-', '/', '؛', '#', '*', '\\n', '\\\"',\n",
        "                ']', '[', '«', '»', '٪', '+', '٠', '\\\\', '\\\"', '_', '\\'']\n",
        "\n",
        "\n",
        "sign = ['%', '@', '_', \"\\\"\", '$', '&', ',', '\"', '>', '<', '|',\n",
        "                         '­', ';', 'é', '=', '+', '?']\n",
        "\n",
        "numbers = ['0', '1', '2', '3', '4',\n",
        "                         '5', '6', '7', '8', '9', '۰', '۱', '۲', '۳', '۴', '۵', '۶', '۷', '۸', '۹']\n",
        "\n",
        "english_chars = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O',\n",
        "                   'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o',\n",
        "                   'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
        "\n",
        "escape_chars = ['\\u200c', '\\u200b', '\\u200f']\n",
        "\n",
        "clear_list = [sign, numbers, english_chars, escape_chars]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jvxEiju9lvcT"
      },
      "outputs": [],
      "source": [
        "def split_word(word):\n",
        "  return [c for c in word]\n",
        "\n",
        "def clear_content(content_to_clear):\n",
        "    \n",
        "    for p in punctuation:\n",
        "        content_to_clear = content_to_clear.replace(p, \" \")\n",
        "    \n",
        "    for i in clear_list:\n",
        "\n",
        "      for n in i:\n",
        "        content_to_clear = content_to_clear.replace(n, '')\n",
        "\n",
        "    return content_to_clear"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "emDk-352oPc-"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "with open(\"/content/gdrive/MyDrive/IR_data_news_12k.json\", 'r', encoding='utf-8') as readfile:\n",
        "  data = json.load(readfile)\n",
        "    \n",
        "positional_index = {}\n",
        "doc_id_title = {}\n",
        "term_frq_per_doc = []\n",
        "term_frq = {}\n",
        "doc_id_url = {}\n",
        "\n",
        "\n",
        "\n",
        "stop_words = stopwords_list()\n",
        "stemmer = Stemmer()\n",
        "lemmatizer = Lemmatizer()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4J4G75-soSBC"
      },
      "outputs": [],
      "source": [
        "def Zipfs_law(term_frq):\n",
        "    print(\"hi\")\n",
        "    term_frq = dict(sorted(term_frq.items(), key=lambda item: item[1], reverse=True))\n",
        "    log_rank = np.log10(np.linspace(1, len(term_frq), len(term_frq)))\n",
        "    log_cf = np.log10(np.array(list(term_frq.values())))\n",
        "    plt.plot(log_rank, log_cf, 'b')\n",
        "    term_frq_values = list(term_frq.values())\n",
        "    cf1 = term_frq_values[0]\n",
        "    log_i = np.log10(np.linspace(2, len(term_frq), len(term_frq) - 1))\n",
        "    log_cfi = np.log10(cf1) - log_i\n",
        "    plt.plot(log_i, log_cfi, 'r')\n",
        "\n",
        "    #plt.title(\"Checking Zipf's Law after removing stop words\")\n",
        "    plt.title(\"Checking Zipf's Law before removing stop words\")\n",
        "    plt.legend([\"Actual Values\", \"Zipf's Law\"])\n",
        "    plt.xlabel(\"log10 rank\")\n",
        "    plt.ylabel(\"log10 cf\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1d_nSLqHoUko"
      },
      "outputs": [],
      "source": [
        "def heaps_law(p1, p2, p3, p4, p5, p6, p7, p8, p9, p10):\n",
        "    log_M = np.log10([p2, p4, p6, p8])\n",
        "    log_T = np.log10([p1, p3, p5, p7])\n",
        "\n",
        "    main_M = p10\n",
        "    main_T = p9\n",
        "\n",
        "    plt.plot(log_T, log_M, 'b')\n",
        "    [b, log_k] = np.polyfit(log_T, log_M, 1)\n",
        "    plt.plot(log_T, b * log_T + log_k, 'r')\n",
        "\n",
        "    plt.title(\"Checking Heaps' Law after finding roots\")\n",
        "    plt.legend([\"Actual Values after finding roots\", \"Heaps' Law after finding roots\"])\n",
        "    plt.xlabel(\"log10 T\")\n",
        "    plt.ylabel(\"log10 M\")\n",
        "    plt.show()\n",
        "\n",
        "    log_main_T = np.log10(main_T)\n",
        "    predict_log_main_M = log_k + b * log_main_T\n",
        "    predict_main_M = np.power(10, predict_log_main_M)\n",
        "\n",
        "    k = np.power(10, log_k)\n",
        "\n",
        "    print(\"k = \", k)\n",
        "    print(\"b = \", b)\n",
        "    print(\"Number of Tokens: \", main_T)\n",
        "    print(\"Predicted Size of Vocabulary : \", predict_main_M)\n",
        "    print(\"Actual Size of Dictionary : \", main_M)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GE4sd4YEpYJS"
      },
      "outputs": [],
      "source": [
        "count = 0\n",
        "doc_id = -1\n",
        "for row in data:\n",
        "    dictionary = dict()\n",
        "    term_position = dict()\n",
        "    count = count + 1\n",
        "\n",
        "    doc_id = doc_id + 1\n",
        "\n",
        "    doc_id_title[doc_id] = data[row]['title']\n",
        "    doc_id_url[doc_id] = data[row]['url']\n",
        "\n",
        "\n",
        "    content = data[row]['content']\n",
        "\n",
        "    content = clear_content(content)\n",
        "\n",
        "    terms = word_tokenize(content)\n",
        "    counter = 0\n",
        "\n",
        "\n",
        "\n",
        "    # removing stop words\n",
        "    for words in stop_words:\n",
        "        for term in terms:\n",
        "            if term == words:\n",
        "                counter = counter + 1\n",
        "        for i in range(counter):\n",
        "            terms.remove(words)\n",
        "        counter = 0\n",
        "\n",
        "    final_terms = list()\n",
        "\n",
        "    for term in terms:\n",
        "    #res = stemmer.stem(term)\n",
        "    #res = lemmatizer.lemmatize(res)\n",
        "    #final_terms.append(res)\n",
        "    # used while ignoring stemming for heaps law calculations\n",
        "      final_terms.append(term)\n",
        "\n",
        "    for i in final_terms:\n",
        "        position_list = list()\n",
        "        if i in dictionary:\n",
        "            pass\n",
        "        else:\n",
        "            position_counter = 0\n",
        "            for j in final_terms:\n",
        "                position_counter = position_counter + 1\n",
        "                if i == j:\n",
        "                    frq = dictionary.get(i, 0)\n",
        "                    dictionary[i] = frq + 1\n",
        "                    position_list = term_position.get(i, [])\n",
        "                    position_list.append(position_counter)\n",
        "                    term_position[i] = position_list\n",
        "\n",
        "    term_frq_per_doc.append(dictionary)\n",
        "\n",
        "    # creating positional index\n",
        "    for term in term_position.keys():\n",
        "        if term in positional_index.keys():\n",
        "            dic1 = positional_index.get(term, {})\n",
        "            dic1[doc_id] = term_position[term]\n",
        "            positional_index[term] = dic1\n",
        "            # pass\n",
        "        else:\n",
        "            doc_list = dict()\n",
        "            doc_list[doc_id] = term_position[term]\n",
        "            positional_index[term] = doc_list\n",
        "\n",
        "positional_index = collections.OrderedDict(sorted(positional_index.items()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O21EaP8Mc48I"
      },
      "outputs": [],
      "source": [
        "frequency = 0\n",
        "for term in positional_index.keys():\n",
        "    for words_per_doc in term_frq_per_doc:\n",
        "        if term in words_per_doc:\n",
        "            frequency = frequency + words_per_doc[term]\n",
        "    term_frq[term] = frequency\n",
        "    frequency = 0\n",
        "\n",
        "the_first_500_tokens = list()\n",
        "the_first_1000_tokens = list()\n",
        "the_first_1500_tokens = list()\n",
        "the_first_2000_tokens = list()\n",
        "for term in positional_index:\n",
        "    for docid in positional_index[term]:\n",
        "        if docid < 500:\n",
        "            the_first_500_tokens.append(term)\n",
        "\n",
        "for term in positional_index:\n",
        "    for docid in positional_index[term]:\n",
        "        if docid < 1000:\n",
        "            the_first_1000_tokens.append(term)\n",
        "\n",
        "for term in positional_index:\n",
        "    for docid in positional_index[term]:\n",
        "        if docid < 1500:\n",
        "            the_first_1500_tokens.append(term)\n",
        "\n",
        "for term in positional_index:\n",
        "    for docid in positional_index[term]:\n",
        "        if docid < 2000:\n",
        "            the_first_2000_tokens.append(term)\n",
        "\n",
        "\n",
        "the_first_500_terms = set(the_first_500_tokens)\n",
        "the_first_1000_terms = set(the_first_1000_tokens)\n",
        "the_first_1500_terms = set(the_first_1500_tokens)\n",
        "the_first_2000_terms = set(the_first_2000_tokens)\n",
        "\n",
        "all_tokens = positional_index\n",
        "all_tokens_len = 0\n",
        "for term in positional_index:\n",
        "    all_tokens_len += len(positional_index[term].values())\n",
        "\n",
        "all_terms = len(positional_index.keys())\n",
        "\n",
        "p1 = len(the_first_500_tokens)\n",
        "p2 = len(the_first_500_terms)\n",
        "p3 = len(the_first_1000_tokens)\n",
        "p4 = len(the_first_1000_terms)\n",
        "p5 = len(the_first_1500_tokens)\n",
        "p6 = len(the_first_1500_terms)\n",
        "p7 = len(the_first_2000_tokens)\n",
        "p8 = len(the_first_2000_terms)\n",
        "p9 = all_tokens_len\n",
        "p10 = all_terms\n",
        "\n",
        "\n",
        "print(\"Tokens of the first 500 docs \")\n",
        "print(p1)\n",
        "print(\"terms of the first 500 docs \")\n",
        "print(p2)\n",
        "print(\"Tokens of the first 1000 docs \")\n",
        "print(p3)\n",
        "print(\"terms of the first 1000 docs \")\n",
        "print(p4)\n",
        "print(\"Tokens of the first 1500 docs \")\n",
        "print(p5)\n",
        "print(\"terms of the first 1500 docs \")\n",
        "print(p6)\n",
        "print(\"Tokens of the first 2000 docs \")\n",
        "print(p7)\n",
        "print(\"terms of the first 2000 docs \")\n",
        "print(p8)\n",
        "print(\"Tokens of all the docs\")\n",
        "print(p9)\n",
        "print(\"terms of all the docs\")\n",
        "print(p10)\n",
        "\n",
        "Zipfs_law(term_frq)\n",
        "\n",
        "heaps_law(p1, p2, p3, p4, p5, p6, p7, p8, p9, p10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1CtxjsSVo5b3"
      },
      "outputs": [],
      "source": [
        "with open(\"/content/gdrive/MyDrive/positional_index.json\", \"w\") as write_file:\n",
        "   json.dump(positional_index, write_file)\n",
        "\n",
        "with open(\"/content/gdrive/MyDrive/doc_id_title.json\", \"w\") as write_file:\n",
        "   json.dump(doc_id_title, write_file)\n",
        "\n",
        "with open(\"/content/gdrive/MyDrive/doc_id_url.json\", \"w\") as write_file:\n",
        "   json.dump(doc_id_url, write_file)\n",
        "\n",
        "with open(\"/content/gdrive/MyDrive/term_frq.json\", \"w\") as write_file:\n",
        "   json.dump(term_frq, write_file)\n",
        "\n",
        "with open(\"/content/gdrive/MyDrive/term_frq_per_doc.json\", \"w\") as write_file:\n",
        "   json.dump(term_frq_per_doc, write_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZdXkp3h93AKj"
      },
      "outputs": [],
      "source": [
        "stop_words = stopwords_list()\n",
        "stemmer = Stemmer()\n",
        "lemmatizer = Lemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oCGYtBbH3oZ2"
      },
      "outputs": [],
      "source": [
        "with open(\"/content/gdrive/MyDrive/positional_index.json\", \"r\") as read_file:\n",
        "    positional_index = json.load(read_file)\n",
        "\n",
        "with open(\"/content/gdrive/MyDrive/doc_id_title.json\", \"r\") as read_file:\n",
        "    doc_id_title = json.load(read_file)\n",
        "\n",
        "with open(\"/content/gdrive/MyDrive/doc_id_url.json\", \"r\") as read_file:\n",
        "    doc_id_url = json.load(read_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQoqtf5x385h"
      },
      "outputs": [],
      "source": [
        "query = \"دانشگاه ~امیرکبیر\"\n",
        "\n",
        "query = clear_content(query)\n",
        "\n",
        "query_list = query.split()\n",
        "new_list = []\n",
        "remove_words = []\n",
        "\n",
        "for i in query_list:\n",
        "  if i[0] == \"~\":\n",
        "    remove_words.append(i.replace('~', ''))\n",
        "    continue\n",
        "  new_list.append(i)\n",
        "\n",
        "query_list = new_list\n",
        "\n",
        "q_l = list(query_list)\n",
        "query_list.clear()\n",
        "for term in q_l:\n",
        "    res = stemmer.stem(term)\n",
        "    res = lemmatizer.lemmatize(res)\n",
        "    query_list.append(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xRMu4KsY3_Ol"
      },
      "outputs": [],
      "source": [
        "# One Word Query\n",
        "if len(query_list) == 1:\n",
        "    docs_to_remove = list()\n",
        "    for term in remove_words:\n",
        "        docs_to_remove = list(positional_index[term].keys())\n",
        "    if query_list[0] in positional_index.keys():\n",
        "        for i in positional_index[query_list[0]]:\n",
        "            if i not in docs_to_remove:\n",
        "                print(doc_id_title[i])\n",
        "                print(doc_id_url[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pOGSBkGIFJ3w"
      },
      "outputs": [],
      "source": [
        "states = []\n",
        "for i in range(0, 2**len(query_list)):\n",
        "  new_list = query_list.copy()\n",
        "  dictionary = dict()\n",
        "  binary_format = split_word(format(i, \"b\"))\n",
        "  while len(binary_format) != len(query_list):\n",
        "    binary_format.insert(0, '0')\n",
        "\n",
        "  for j in range(0, len(binary_format)):\n",
        "    if binary_format[j] == '1':\n",
        "      new_list[j] = '0'\n",
        "  zero_count = 0\n",
        "  for n in new_list:\n",
        "    if n == '0':\n",
        "      zero_count += 1\n",
        "  for i in range(zero_count):\n",
        "    new_list.remove('0')\n",
        "  dictionary[len(new_list)] = new_list\n",
        "  states.append(dictionary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IER9V1A-3s7J"
      },
      "outputs": [],
      "source": [
        "count = len(query_list)\n",
        "counter = 0\n",
        "docs_to_retrieve = []\n",
        "title_results = []\n",
        "while count != 1:\n",
        "    for dict1 in states:\n",
        "        k = list(dict1.keys())\n",
        "        if k[0] == count:\n",
        "            # Start\n",
        "            positional_postings_lists = list()\n",
        "            doc_id_list = list()\n",
        "            position_list = list()\n",
        "            dict1_values = list(dict1.values())\n",
        "            word_list = list(dict1_values[0])\n",
        "            for term in word_list:\n",
        "                id_position_per_term = dict()\n",
        "                for row in positional_index:\n",
        "                    if row == term:\n",
        "                        doc_ids_list = list(positional_index[row].keys())\n",
        "                        for word_doc in doc_ids_list:\n",
        "                            id_position_per_term[word_doc] = list(positional_index[row][word_doc])\n",
        "                positional_postings_lists.append(id_position_per_term)\n",
        "\n",
        "            # Start Algorithm\n",
        "            end = 0\n",
        "            doc_id = 0\n",
        "            flag = 0\n",
        "            flag3 = 0\n",
        "            check = 1\n",
        "            keys = list(positional_postings_lists[0].keys())\n",
        "            if len(keys) != 0:\n",
        "                last_doc_id_term_1 = keys[len(keys) - 1]\n",
        "                last_doc_id_term_1 = int(last_doc_id_term_1)\n",
        "            else:\n",
        "                end = 1\n",
        "\n",
        "            while True:\n",
        "                if end == 1:\n",
        "                    end = 0\n",
        "                    break\n",
        "                if doc_id > last_doc_id_term_1:\n",
        "                    break\n",
        "                d_i = 0\n",
        "                for d_i in positional_postings_lists[0]:\n",
        "                    d_i = int(d_i)\n",
        "                    if doc_id > d_i:\n",
        "                        continue\n",
        "                    for term in positional_postings_lists:\n",
        "                        if term == positional_postings_lists[0]:\n",
        "                            continue\n",
        "                        d_i = str(d_i)\n",
        "                        if d_i in term.keys():\n",
        "                            pass\n",
        "                        else:\n",
        "                            flag = -1\n",
        "                    if flag == -1:\n",
        "                        flag = 0\n",
        "                        check = check + 1\n",
        "                        continue\n",
        "                    else:\n",
        "                        flag3 = 1\n",
        "                        d_i = int(d_i)\n",
        "                        doc_id = d_i\n",
        "                        break\n",
        "\n",
        "                d_i = int(d_i)\n",
        "\n",
        "                if check == (len(positional_postings_lists[0]) + 1):\n",
        "                    check = 1\n",
        "                    break\n",
        "\n",
        "                if flag3 == 0 and doc_id == last_doc_id_term_1:\n",
        "                    break\n",
        "\n",
        "                if flag3 == 0 and d_i == last_doc_id_term_1:\n",
        "                    break\n",
        "                flag3 = 0\n",
        "\n",
        "                d_i = str(d_i)\n",
        "\n",
        "                doc_id = str(doc_id)\n",
        "                if doc_id not in positional_postings_lists[0].keys():\n",
        "                    break\n",
        "                term_1_positions_per_doc_id = positional_postings_lists[0][doc_id]\n",
        "                flag1 = 0\n",
        "                flag2 = 0\n",
        "                list_found_docs = list()\n",
        "                for o in term_1_positions_per_doc_id:\n",
        "                    o = int(o)\n",
        "                    pos = o\n",
        "                    for term in positional_postings_lists:\n",
        "                        if term == positional_postings_lists[0]:\n",
        "                            continue\n",
        "                        pos = pos + 1\n",
        "                        v = term[doc_id]\n",
        "                        pos = str(pos)\n",
        "                        if pos in v:\n",
        "                            pass\n",
        "                        else:\n",
        "                            flag1 = -1\n",
        "                        pos = int(pos)\n",
        "                    if flag == -1:\n",
        "                        flag = 0\n",
        "                        continue\n",
        "                    else:\n",
        "                        list_found_docs.append(doc_id)\n",
        "                        docs_to_retrieve.append(doc_id)\n",
        "                        break\n",
        "\n",
        "                doc_id = int(doc_id)\n",
        "                doc_id = doc_id + 1\n",
        "\n",
        "            # End Algorithm\n",
        "            # End\n",
        "    count = count - 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2eeN5Mrv2uL7"
      },
      "outputs": [],
      "source": [
        "for term in remove_words:\n",
        "    docs_to_remove = list(positional_index[term].keys())\n",
        "    for d in docs_to_remove:\n",
        "        if d in docs_to_retrieve:\n",
        "            docs_to_retrieve.remove(d)\n",
        "\n",
        "for i in docs_to_retrieve:\n",
        "    print(doc_id_title[i])\n",
        "    print(doc_id_url[i])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g2L3XB0fdaUa"
      },
      "outputs": [],
      "source": [
        "def tfidf(tf, df, N):\n",
        "    if tf * df == 0:\n",
        "        return 0.\n",
        "    return (1 + np.log10(tf)) * np.log10(1. * N / df)\n",
        "\n",
        "\n",
        "with open(\"/content/gdrive/MyDrive/positional_index.json\", \"r\") as read_file:\n",
        "    positional_index = json.load(read_file)\n",
        "\n",
        "with open(\"/content/gdrive/MyDrive/doc_id_title.json\", \"r\") as read_file:\n",
        "    doc_id_title = json.load(read_file)\n",
        "\n",
        "with open(\"/content/gdrive/MyDrive/term_frq_per_doc.json\", \"r\") as read_file:\n",
        "    term_frq_per_doc = json.load(read_file)\n",
        "\n",
        "term_df = dict()\n",
        "for term in positional_index:\n",
        "    term_df[term] = len(positional_index[term].keys())\n",
        "\n",
        "N = len(doc_id_title.keys())\n",
        "lengths = np.zeros(N)\n",
        "for term in positional_index:\n",
        "    df = term_df[term]\n",
        "    docs = positional_index[term].keys()\n",
        "    for doc_id in docs:\n",
        "        doc_id = int(doc_id)\n",
        "        if term not in term_frq_per_doc[doc_id]:\n",
        "            continue\n",
        "        tf = term_frq_per_doc[doc_id][term]\n",
        "        weight = tfidf(tf, df, N)\n",
        "        lengths[doc_id] += weight ** 2\n",
        "lengths = np.sqrt(lengths)\n",
        "lengths = list(lengths)\n",
        "with open(\"/content/gdrive/MyDrive/lengths.json\", \"w\") as write_file:\n",
        "    json.dump(lengths, write_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVHxXUqUdaUb"
      },
      "outputs": [],
      "source": [
        "with open(\"/content/gdrive/MyDrive/doc_id_title.json\", \"r\") as read_file:\n",
        "    doc_id_title = json.load(read_file)\n",
        "\n",
        "with open(\"/content/gdrive/MyDrive/positional_index.json\", \"r\") as read_file:\n",
        "    positional_index = json.load(read_file)\n",
        "\n",
        "with open(\"/content/gdrive/MyDrive/term_frq_per_doc.json\", \"r\") as read_file:\n",
        "    term_frq_per_doc = json.load(read_file)\n",
        "\n",
        "r = input(\"Enter r: \")\n",
        "r = int(r)\n",
        "postings_lists = dict()\n",
        "\n",
        "for term in positional_index:\n",
        "    doc_id_tf = dict()\n",
        "    for doc_id in range(0, len(doc_id_title)):\n",
        "        if term in term_frq_per_doc[doc_id]:\n",
        "            doc_id_tf[doc_id] = term_frq_per_doc[doc_id][term]\n",
        "\n",
        "    res = sorted(doc_id_tf.items(), key=operator.itemgetter(1), reverse=True)[:r]\n",
        "    postings_lists[term] = [t[0] for t in res]\n",
        "\n",
        "with open(\"/content/gdrive/MyDrive/postings_lists.json\", \"w\") as write_file:\n",
        "    json.dump(postings_lists, write_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vuK0hP6j4Glb"
      },
      "outputs": [],
      "source": [
        "with open(\"/content/gdrive/MyDrive/positional_index.json\", \"r\") as read_file:\n",
        "    positional_index = json.load(read_file)\n",
        "\n",
        "with open(\"/content/gdrive/MyDrive/doc_id_title.json\", \"r\") as read_file:\n",
        "    doc_id_title = json.load(read_file)\n",
        "\n",
        "with open(\"/content/gdrive/MyDrive/term_frq_per_doc.json\", \"r\") as read_file:\n",
        "    term_frq_per_doc = json.load(read_file)\n",
        "\n",
        "with open(\"/content/gdrive/MyDrive/doc_id_url.json\", \"r\") as read_file:\n",
        "    doc_id_url = json.load(read_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3tSrgETd4UOP"
      },
      "outputs": [],
      "source": [
        "def split(word):\n",
        "    return [char for char in word]\n",
        "\n",
        "\n",
        "def query_parser(query):\n",
        "\n",
        "    query = clear_content(query)\n",
        "\n",
        "    query_list = query.split()\n",
        "\n",
        "\n",
        "    for words in stop_words:\n",
        "        for term in query_list:\n",
        "            if term == words:\n",
        "                query_list.remove(words)\n",
        "\n",
        "    q_l = list(query_list)\n",
        "    query_list.clear()\n",
        "    for term in q_l:\n",
        "        res = stemmer.stem(term)\n",
        "        res = lemmatizer.lemmatize(res)\n",
        "        query_list.append(res)\n",
        "    return query_list\n",
        "\n",
        "\n",
        "def tfidf(tf, df, N):\n",
        "    if tf * df == 0:\n",
        "        return 0.\n",
        "    return (1 + np.log10(tf)) * np.log10(1. * N / df)\n",
        "\n",
        "def champion_lists_create(status, postings_list):\n",
        "    if status == False:\n",
        "        for term in positional_index:\n",
        "            postings_list[term] = list(positional_index[term].keys())\n",
        "        return postings_list\n",
        "    else:\n",
        "        with open(\"/content/gdrive/MyDrive/postings_lists.json\", \"r\") as read_file:\n",
        "            postings_list = json.load(read_file)\n",
        "        return postings_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l8oS-Ug2daUb"
      },
      "outputs": [],
      "source": [
        "term_df = dict()\n",
        "for term in positional_index:\n",
        "    term_df[term] = len(positional_index[term].keys())\n",
        "\n",
        "postings_list = dict()\n",
        "# showing top 10 results\n",
        "k = 15\n",
        "\n",
        "# if champion_lists_status is set True we use Champion lists if is set to False we use the default Postings lists\n",
        "champion_lists_status = True\n",
        "postings_list = champion_lists_create(champion_lists_status, postings_list)\n",
        "query = \"لیگ فوتبال آسیا\"\n",
        "query = query_parser(query)\n",
        "N = len(doc_id_title.keys())\n",
        "\n",
        "with open(\"/content/gdrive/MyDrive/lengths.json\", \"r\") as read_file:\n",
        "    lengths = json.load(read_file)\n",
        "\n",
        "scores = dict()\n",
        "for qt in query:\n",
        "    df = term_df[qt]\n",
        "    qt_tf = query.count(qt)\n",
        "    qt_weight = tfidf(df, qt_tf, N)\n",
        "    qt_postings = postings_list[qt]\n",
        "    for doc_id in qt_postings:\n",
        "        doc_id = int(doc_id)\n",
        "        if lengths[doc_id] == 0:\n",
        "            continue\n",
        "        doc_tf = term_frq_per_doc[doc_id][qt]\n",
        "        doc_weight = tfidf(df, doc_tf, N)\n",
        "        try:\n",
        "            scores[doc_id] += qt_weight * doc_weight\n",
        "        except KeyError:\n",
        "            scores[doc_id] = qt_weight * doc_weight\n",
        "\n",
        "\n",
        "for doc_id in scores:\n",
        "    scores[doc_id] /= lengths[doc_id]\n",
        "\n",
        "scores = dict(sorted(scores.items(), key=operator.itemgetter(1), reverse=True))\n",
        "docs_to_retrieve = list(scores.keys())[:k]\n",
        "for i in docs_to_retrieve:\n",
        "    i = str(i)\n",
        "    print(f\"{doc_id_url[i]}\\n\"\n",
        "          f\"doc id: \\n{i}\\n\"\n",
        "          f\"Title: \\n{doc_id_title[i]}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}